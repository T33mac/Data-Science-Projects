#install.packages('tidyr')
#install.packages("readr")
#install.packages('purrr')
#install.packages('stringr')
#install.packages('forcats')
install.packages('tidyverse')
#install.packages('tibble')
#install.packages('tidyr')
#install.packages("readr")
#install.packages('purrr')
#install.packages('stringr')
#install.packages('forcats')
#install.packages('tidyverse')
install.packages('reshape2')
#install.packages('tidyverse')
#install.packages('reshape2')
install.packages('reshape2')
#Load libraries
library(tidyverse)
library(reshape2)
#Load data
housing <- read.csv('~/housing.csv')
#Load data
housing <- read.csv('housing.csv')
#Load data
housing <- read.csv('/housing.csv')
#Set working directory
setwd('~/Machine_Learning_R')
#Load data
housing <- read.csv('housing.csv')
head(housing)
head(housing)
summary(housing)
#Set working directory
setwd('~/Machine_Learning_R')
#Load libraries
library(tidyverse)
library(reshape2)
#Load data
housing <- read.csv('housing.csv')
head(housing)
summary(housing)
par(mfrow=c(2,5))
colnames(housing)
ggplot(data = melt(housing), mapping = aes(x = value)) +
geom_histogram(bins = 30) + facet_wrap(~variable, scales = 'free_x')
#Set working directory
setwd('~/Machine_Learning_R')
#Load libraries
library(tidyverse)
library(reshape2)
#Load data
housing <- read.csv('housing.csv')
head(housing)
summary(housing)
par(mfrow=c(2,5))
colnames(housing)
ggplot(data = melt(housing), mapping = aes(x = value)) +
geom_histogram(bins = 30) + facet_wrap(~variable, scales = 'free_x')
?par
housing$total_bedrooms[is.na(housing$total_bedrooms)] = median(housing$total_bedrooms , na.rm = TRUE)
housing$mean_bedrooms = housing$total_bedrooms/housing$households
housing$mean_rooms = housing$total_rooms/housing$households
drops = c('total_bedrooms', 'total_rooms')
housing = housing[ , !(names(housing) %in% drops)]
#view
head(housing)
#List ocean prox categories
categories = unique(housing$ocean_proximity)
setwd('~/Machine_Learning_R')
library(tidyverse)
library(reshape2)
housing <- read.csv('housing.csv')
head(housing)
summary(housing)
?par
?par
par(mfrow=c(2,5))
colnames(housing)
ggplot(data = melt(housing), mapping = aes(x = value)) +
geom_histogram(bins = 30) + facet_wrap(~variable, scales = 'free_x')
housing$total_bedrooms[is.na(housing$total_bedrooms)] = median(housing$total_bedrooms , na.rm = TRUE)
housing$mean_bedrooms = housing$total_bedrooms/housing$households
housing$mean_rooms = housing$total_rooms/housing$households
housing$mean_bedrooms = housing$total_bedrooms/housing$households
housing$mean_rooms = housing$total_rooms/housing$households
drops = c('total_bedrooms', 'total_rooms')
housing = housing[ , !(names(housing) %in% drops)]
head(housing)
categories = unique(housing$ocean_proximity)
cat_housing = data.frame(ocean_proximity = housing$ocean_proximity)
categories = unique(housing$ocean_proximity)
categories
cat_housing = data.frame(ocean_proximity = housing$ocean_proximity)
cat_housing = data.frame(ocean_proximity = housing$ocean_proximity)
for(cat in categories){
cat_housing[,cat] = rep(0, times= nrow(cat_housing))
}
head(cat_housing)
tail(cat_housing)
for(i in 1:length(cat_housing$ocean_proximity)){
cat = as.character(cat_housing$ocean_proximity[i])
cat_housing[,cat][i] = 1
}
head(cat_housing)
tail(cat_housing)
cat_columns = names(cat_housing)
keep_columns = cat_columns[cat_columns != 'ocean_proximity']
cat_housing = select(cat_housing,one_of(keep_columns))
tail(cat_housing)
drops = c('ocean_proximity', 'median_house_value')
housing_num = housing[, !(names(housing) %in% drops)]
drops = c('ocean_proximity', 'median_house_value')
housing_num = housing[, !(names(housing) %in% drops)]
head(housing_num)
scaled_housing_num = scale(housing_num)
head(scaled_housing_num)
cleaned_housing = cbind(cat_housing, scaled_housing_num,          median_house_value=housing$median_house_value)
head(cleaned_housing)
cleaned_housing = cbind(cat_housing, scaled_housing_num,          median_house_value=housing$median_house_value)
head(cleaned_housing)
set.seed(1738)
set.seed(1738)
set.seed(1738)
sample = sample.int(n = nrow(cleaned_housing),
size = floor(.8*nrow(cleaned_housing)),
replace = F)
train = cleaned_housing[sample,]#just the samples
test = cleaned_housing[-sample,]#not the samples
head(train)
nrow(train) + nrow(test) == nrow(cleaned_housing)
library(boot)
?cv.glm # note the K option for K fold cross validation
library(boot)
?cv.glm # note the K option for K fold cross validation
glm_house = glm(median_house_value~median_income+
mean_rooms+population,
data=cleaned_housing)
k_fold_cv_error = cv.glm(cleaned_housing, glm_house, K=5)
k_fold_cv_error$delta
glm_cv_rmse = sqrt(k_fold_cv_error$delta)[1]
glm_cv_rmse #off by about $83,000... it is a start
names(glm_house)
glm_house$coefficients
library(randomForest)
?randomForest
library(randomForest)
?randomForest
names(train)
set.seed(1738)
train_y = train[,"median_house_value"]
train_x = train[, names(train) !="median_house_value"]
head(train_y)
head(train_x)
rf_model = randomForest(train_x, y = train_y,
ntree = 500,
importance = TRUE)
rf_model
names(rf_model)
rf_model$importance
oob_prediction = predict(rf_model) #leaving out a data source forces OOB predictions
train_mse = mean(as.numeric((oob_prediction - train_y)^2))
oob_rmse = sqrt(train_mse)
oob_rmse
test_y = test[,'median_house_value']
test_x = test[, names(test) !='median_house_value']
y_pred = predict(rf_model , test_x)
test_mse = mean(((y_pred - test_y)^2))
test_rmse = sqrt(test_mse)
test_rmse
setwd('~/Machine_Learning_R')
library(tidyverse)
library(reshape2)
housing <- read.csv('housing.csv')
head(housing)
housing <- read.csv('housing.csv')
head(housing)
summary(housing)
?par
par(mfrow=c(2,5))
colnames(housing)
ggplot(data = melt(housing), mapping = aes(x = value)) +
geom_histogram(bins = 30) + facet_wrap(~variable, scales = 'free_x')
par(mfrow=c(2,5))
colnames(housing)
ggplot(data = melt(housing), mapping = aes(x = value)) +
geom_histogram(bins = 30) + facet_wrap(~variable, scales = 'free_x')
housing$total_bedrooms[is.na(housing$total_bedrooms)] = median(housing$total_bedrooms , na.rm = TRUE)
housing$mean_bedrooms = housing$total_bedrooms/housing$households
housing$mean_rooms = housing$total_rooms/housing$households
drops = c('total_bedrooms', 'total_rooms')
housing = housing[ , !(names(housing) %in% drops)]
head(housing)
categories = unique(housing$ocean_proximity)
categories
cat_housing = data.frame(ocean_proximity = housing$ocean_proximity)
for(cat in categories){
cat_housing[,cat] = rep(0, times= nrow(cat_housing))
}
head(cat_housing)
tail(cat_housing)
for(cat in categories){
cat_housing[,cat] = rep(0, times= nrow(cat_housing))
}
head(cat_housing)
tail(cat_housing)
for(i in 1:length(cat_housing$ocean_proximity)){
cat = as.character(cat_housing$ocean_proximity[i])
cat_housing[,cat][i] = 1
}
head(cat_housing)
tail(cat_housing)
cat_columns = names(cat_housing)
keep_columns = cat_columns[cat_columns != 'ocean_proximity']
cat_housing = select(cat_housing,one_of(keep_columns))
tail(cat_housing)
drops = c('ocean_proximity', 'median_house_value')
housing_num = housing[, !(names(housing) %in% drops)]
head(housing_num)
scaled_housing_num = scale(housing_num)
head(scaled_housing_num)
cleaned_housing = cbind(cat_housing, scaled_housing_num,          median_house_value=housing$median_house_value)
head(cleaned_housing)
set.seed(1738)
sample = sample.int(n = nrow(cleaned_housing),
size = floor(.8*nrow(cleaned_housing)),
replace = F)
train = cleaned_housing[sample,]#just the samples
test = cleaned_housing[-sample,]#not the samples
head(train)
nrow(train) + nrow(test) == nrow(cleaned_housing)
library(boot)
?cv.glm # note the K option for K fold cross validation
glm_house = glm(median_house_value~median_income+
mean_rooms+population,
data=cleaned_housing)
k_fold_cv_error = cv.glm(cleaned_housing, glm_house, K=5)
glm_cv_rmse = sqrt(k_fold_cv_error$delta)[1]
glm_cv_rmse #off by about $83,000... it is a start
names(glm_house)
glm_house$coefficients
library(randomForest)
?randomForest
names(train)
set.seed(1738)
train_y = train[,"median_house_value"]
train_x = train[, names(train) !="median_house_value"]
head(train_y)
head(train_x)
rf_model = randomForest(train_x, y = train_y,
ntree = 500,
importance = TRUE)
names(rf_model)
rf_model$importance
oob_prediction = predict(rf_model) #leaving out a data source forces OOB predictions
test_y = test[,'median_house_value']
test_x = test[, names(test) !='median_house_value']
y_pred = predict(rf_model , test_x)
test_mse = mean(((y_pred - test_y)^2))
test_rmse = sqrt(test_mse)
test_rmse
cities <- read.csv('cal_cities_lat_long.csv')
head(cities)
cities <- read.csv('cal_cities_lat_long.csv')
head(cities)
summary(cities)
head(train)
head(housing)
housing_loc <- c(housing$latitude, housing$longitude,
housing$median_house_value)
head(housing_loc)
housing_loc <- housing(housing$latitude, housing$longitude,
housing$median_house_value)
housing_loc <- housing(latitude, longitude,
median_house_value)
housing_loc = housing[housing$latitude,housing$longitude, housing$median_house_value]
head(housing_loc)
drops_loc = c('ocean_proximity', 'median_house_value',
'housing_median_age', 'population',
'mean_bedrooms', 'mean_rooms',
'housing_median_age', 'households')
housing_loc = housing[, !(names(housing) %in% drops_loc)]
head(housing_num)
drops_loc = c('ocean_proximity', 'median_house_value',
'housing_median_age', 'population',
'mean_bedrooms', 'mean_rooms',
'housing_median_age', 'households')
housing_loc = housing[, !(names(housing) %in% drops_loc)]
head(housing_loc)
drops_loc = c('ocean_proximity', 'median_income',
'housing_median_age', 'population',
'mean_bedrooms', 'mean_rooms',
'housing_median_age', 'households')
housing_loc = housing[, !(names(housing) %in% drops_loc)]
head(housing_loc)
for (loc in housing_loc)
if (housing_loc$latitude = cities$Latitude)
#for (loc in housing_loc)
#if (housing_loc$latitude = cities$Latitude)
#housing_loc = housing_loc[,cities$Name]
#head(housing_loc)
class(housing_loc$longitude)
#for (loc in housing_loc)
#if (housing_loc$latitude = cities$Latitude)
#housing_loc = housing_loc[,cities$Name]
#head(housing_loc)
class(housing_loc$longitude)
class(cities$Longitude)
for (loc in housing_loc)
if (housing_loc$latitude = cities$Latitude)
pop <- read.csv('cal_cities_population.csv')
pop <- read.csv('cal_population_cities.csv')
pop <- read.csv('cal_populations_cities.csv')
setwd('~/Machine_Learning_R')
pop <- read.csv('cal_populations_cities.csv')
pops <- read.csv('cal_populations_cities.csv')
pops <- read.csv('cal_populations_city.csv')
head(pops)
drops_pops = c('Incorportation_date', 'pop_april_1980',
'pop_april_2000')
pops = pops[, !(names(pops) %in% drops_pops)]
head(pops)
drops_pops = c('Incorportation_date', 'pop_april_1980',
'pop_april_2000', 'County',
'pop_april_2010')
pops = pops[, !(names(pops) %in% drops_pops)]
head(pops)
cities <- read.csv('cal_cities_lat_long.csv')
cities %>% rename(Name = City)
cities <- read.csv('cal_cities_lat_long.csv')
cities %>% rename(City = Name)
head(cities)
cities <- read.csv('cal_cities_lat_long.csv')
cities %>% rename(City = Name)
head(cities)
tail(cities)
cities <- read.csv('cal_cities_lat_long.csv')
cities <- rename(City = Name)
cities <- read.csv('cal_cities_lat_long.csv')
cities %>% rename(City = Name)
head(cities)
cities <- read.csv('cal_cities_lat_long.csv')
cities %>% rename(City = Name)
cities
cities <- read.csv('cal_cities_lat_long.csv')
cities %>% rename(City = Name)
head(cities)
cities <- read.csv('cal_cities_lat_long.csv')
names(cities)[names(cities) == "Name"] <- "City"
summary(cities)
head(cities)
drops_pops = c('Incorportation_date', 'pop_april_1980',
'pop_april_2000', 'County',
'pop_april_2010')
pops = pops[, !(names(pops) %in% drops_pops)]
head(pops(10))
drops_pops = c('Incorportation_date', 'pop_april_1980',
'pop_april_2000', 'County',
'pop_april_2010')
pops = pops[, !(names(pops) %in% drops_pops)]
head(pops, 10)
head(cities, 10)
cities_m <- merge(cities, pops, by='City')
head(cities_m)
cities_m <- merge(cities, pops, by='City')
head(cities_m, 100)
cities_m$City >= 1000000
print(cities_m$City >= 1000000)
cities_mil <- (cities_m$City >= 1000000)
cities_mil <- (cities_m$City >= 1000000)
head(cities_mil)
cities_mil <- cities_m[cities_m$City >= 1000000]
cities_mil <- cities_m(cities_m$City >= 1000000)
cities_mil <- cities_m[cities_m$City >= 1000000]
cities_mil <- cities_m[cities_m$City >= 1e+06]
cities_merged <- merge(cities, pops, by='City')
head(cities_merged, 100)
cities_mil <- cities_merged[cities_merged$City >= 1e+06]
cities_mil <- cities_merged[ which(cities_merged$City >= 1e+06),]
head(cities_mil)
cities_mil <- cities_merged[
which(cities_merged$pop_april_1990 >= 1e+06),]
head(cities_mil)
cities_mil <- cities_merged[
which(cities_merged$pop_april_1990 >= 8e+05),]
head(cities_mil)
cities_mil <- cities_merged[
which(cities_merged$pop_april_1990 >= 6e+05),]
head(cities_mil)
cities_mil <- cities_merged[
which(cities_merged$pop_april_1990 >= 7e+05),]
head(cities_mil)
install.packages(geosphere)
install.packages(geosphere)
library(geosphere)
library(geosphere)
cities_most <- cities_merged[
which(cities_merged$pop_april_1990 >= 7e+05),]
head(cities_most)
setwd('~/Machine_Learning_R')
setwd('Github/Machine_Learning_R')
setwd('~/Github/Machine_Learning_R')
setwd('~/Machine_Learning_R')
setwd('~/Machine_Learning_R')
setwd('~/Machine_Learning_R')
setwd('~/Machine_Learning_R')
library(tidyverse)
library(reshape2)
housing <- read.csv('housing.csv')
head(housing)
summary(housing)
?par
par(mfrow=c(2,5))
colnames(housing)
ggplot(data = melt(housing), mapping = aes(x = value)) +
geom_histogram(bins = 30) + facet_wrap(~variable, scales = 'free_x')
housing$total_bedrooms[is.na(housing$total_bedrooms)] = median(housing$total_bedrooms , na.rm = TRUE)
housing$mean_bedrooms = housing$total_bedrooms/housing$households
housing$mean_rooms = housing$total_rooms/housing$households
drops = c('total_bedrooms', 'total_rooms')
housing = housing[ , !(names(housing) %in% drops)]
head(housing)
categories = unique(housing$ocean_proximity)
categories
cat_housing = data.frame(ocean_proximity = housing$ocean_proximity)
for(cat in categories){
cat_housing[,cat] = rep(0, times= nrow(cat_housing))
}
head(cat_housing)
tail(cat_housing)
for(i in 1:length(cat_housing$ocean_proximity)){
cat = as.character(cat_housing$ocean_proximity[i])
cat_housing[,cat][i] = 1
}
head(cat_housing)
tail(cat_housing)
cat_columns = names(cat_housing)
keep_columns = cat_columns[cat_columns != 'ocean_proximity']
cat_housing = select(cat_housing,one_of(keep_columns))
tail(cat_housing)
drops = c('ocean_proximity', 'median_house_value')
housing_num = housing[, !(names(housing) %in% drops)]
head(housing_num)
scaled_housing_num = scale(housing_num)
head(scaled_housing_num)
cleaned_housing = cbind(cat_housing, scaled_housing_num,          median_house_value=housing$median_house_value)
head(cleaned_housing)
set.seed(1738)
sample = sample.int(n = nrow(cleaned_housing),
size = floor(.8*nrow(cleaned_housing)),
replace = F)
train = cleaned_housing[sample,]#just the samples
test = cleaned_housing[-sample,]#not the samples
head(train)
nrow(train) + nrow(test) == nrow(cleaned_housing)
library(boot)
?cv.glm # note the K option for K fold cross validation
glm_house = glm(median_house_value~median_income+
mean_rooms+population,
data=cleaned_housing)
k_fold_cv_error = cv.glm(cleaned_housing, glm_house, K=5)
k_fold_cv_error$delta
glm_cv_rmse = sqrt(k_fold_cv_error$delta)[1]
glm_cv_rmse #off by about $83,000... it is a start
names(glm_house)
glm_house$coefficients
library(randomForest)
?randomForest
names(train)
set.seed(1738)
train_y = train[,"median_house_value"]
train_x = train[, names(train) !="median_house_value"]
head(train_y)
head(train_x)
rf_model = randomForest(train_x, y = train_y,
ntree = 500,
importance = TRUE)
names(rf_model)
rf_model$importance
oob_prediction = predict(rf_model) #leaving out a data source forces OOB predictions
train_mse = mean(as.numeric((oob_prediction - train_y)^2))
oob_rmse = sqrt(train_mse)
oob_rmse
test_y = test[,'median_house_value']
test_x = test[, names(test) !='median_house_value']
y_pred = predict(rf_model , test_x)
test_mse = mean(((y_pred - test_y)^2))
test_rmse = sqrt(test_mse)
test_rmse
cities <- read.csv('cal_cities_lat_long.csv')
names(cities)[names(cities) == "Name"] <- "City"
summary(cities)
drops_loc = c('ocean_proximity', 'median_income',
'housing_median_age', 'population',
'mean_bedrooms', 'mean_rooms',
'housing_median_age', 'households')
housing_loc = housing[, !(names(housing) %in% drops_loc)]
head(housing_loc)
pops <- read.csv('cal_populations_city.csv')
head(pops)
drops_pops = c('Incorportation_date', 'pop_april_1980',
'pop_april_2000', 'County',
'pop_april_2010')
pops = pops[, !(names(pops) %in% drops_pops)]
head(pops, 10)
head(cities, 10)
cities_merged <- merge(cities, pops, by='City')
head(cities_merged, 100)
cities_most <- cities_merged[
which(cities_merged$pop_april_1990 >= 7e+05),]
head(cities_most)
library(geosphere)
